{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29aaf64-47fb-4056-adf7-d4fb44bd4727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d63640-ae0e-408f-ba77-e0c73039cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torchaudio\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "import copy\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b6a140-9a94-4128-a17c-90d488002e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchSize():\n",
    "    return 64\n",
    "def epocCount():\n",
    "    return 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3401a98-6bb2-4cac-ae70-296f1688e52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.1+cu102\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import torchvision; print(torchvision.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "218733a5-59b6-48fc-ae53-80978ba23c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "\n",
    "\n",
    "__all__ = [\"Inception3\", \"InceptionOutputs\", \"_InceptionOutputs\", \"Inception_V3_Weights\", \"inception_v3\"]\n",
    "\n",
    "\n",
    "InceptionOutputs = namedtuple(\"InceptionOutputs\", [\"logits\", \"aux_logits\"])\n",
    "InceptionOutputs.__annotations__ = {\"logits\": Tensor, \"aux_logits\": Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _InceptionOutputs set here for backwards compat\n",
    "_InceptionOutputs = InceptionOutputs\n",
    "\n",
    "\n",
    "class Inception3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 152,\n",
    "        aux_logits: bool = True,\n",
    "        transform_input: bool = False,\n",
    "        inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,\n",
    "        init_weights: Optional[bool] = None,\n",
    "        dropout: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if inception_blocks is None:\n",
    "            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn(\n",
    "                \"The default weight initialization of inception_v3 will be changed in future releases of \"\n",
    "                \"torchvision. If you wish to keep the old behavior (which leads to long initialization times\"\n",
    "                \" due to scipy/scipy#11299), please set init_weights=True.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            init_weights = True\n",
    "        if len(inception_blocks) != 7:\n",
    "            raise ValueError(f\"length of inception_blocks should be 7 instead of {len(inception_blocks)}\")\n",
    "        conv_block = inception_blocks[0]\n",
    "        inception_a = inception_blocks[1]\n",
    "        inception_b = inception_blocks[2]\n",
    "        inception_c = inception_blocks[3]\n",
    "        inception_d = inception_blocks[4]\n",
    "        inception_e = inception_blocks[5]\n",
    "        inception_aux = inception_blocks[6]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
    "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
    "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
    "        self.Mixed_6a = inception_b(288)\n",
    "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
    "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
    "        self.AuxLogits: Optional[nn.Module] = None\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = inception_aux(768, num_classes)\n",
    "        self.Mixed_7a = inception_d(768)\n",
    "        self.Mixed_7b = inception_e(1280)\n",
    "        self.Mixed_7c = inception_e(2048)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    stddev = float(m.stddev) if hasattr(m, \"stddev\") else 0.1  # type: ignore\n",
    "                    torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=stddev, a=-2, b=2)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x: Tensor) -> Tensor:\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = self.maxpool2(x)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        aux: Optional[Tensor] = None\n",
    "        if self.AuxLogits is not None:\n",
    "            if self.training:\n",
    "                aux = self.AuxLogits(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = self.avgpool(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = self.dropout(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x: Tensor, aux: Optional[Tensor]) -> InceptionOutputs:\n",
    "        if self.training and self.aux_logits:\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return x  # type: ignore[return-value]\n",
    "\n",
    "    def forward(self, x: Tensor) -> InceptionOutputs:\n",
    "        x = self._transform_input(x)\n",
    "        x, aux = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, pool_features: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionB(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionC(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, channels_7x7: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionD(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionE(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, num_classes: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
    "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
    "        self.conv1.stddev = 0.01  # type: ignore[assignment]\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.fc.stddev = 0.001  # type: ignore[assignment]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # N x 768 x 17 x 17\n",
    "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
    "        # N x 768 x 5 x 5\n",
    "        x = self.conv0(x)\n",
    "        # N x 128 x 5 x 5\n",
    "        x = self.conv1(x)\n",
    "        # N x 768 x 1 x 1\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 768 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 768\n",
    "        x = self.fc(x)\n",
    "        # N x 1000\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "class Inception_V3_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=299, resize_size=342),\n",
    "        meta={\n",
    "            \"num_params\": 27161264,\n",
    "            \"min_size\": (75, 75),\n",
    "            \"categories\": _IMAGENET_CATEGORIES,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#inception-v3\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 77.294,\n",
    "                    \"acc@5\": 93.450,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 5.713,\n",
    "            \"_file_size\": 103.903,\n",
    "            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "#@handle_legacy_interface(weights=(\"pretrained\", Inception_V3_Weights.IMAGENET1K_V1))\n",
    "def inception_v3(*, weights: Optional[Inception_V3_Weights] = None, progress: bool = True, **kwargs: Any) -> Inception3:\n",
    "    \"\"\"\n",
    "    Inception v3 model architecture from\n",
    "    `Rethinking the Inception Architecture for Computer Vision <http://arxiv.org/abs/1512.00567>`_.\n",
    "    .. note::\n",
    "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
    "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Inception_V3_Weights`, optional): The\n",
    "            pretrained weights for the model. See\n",
    "            :class:`~torchvision.models.Inception_V3_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.Inception3``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py>`_\n",
    "            for more details about this class.\n",
    "    .. autoclass:: torchvision.models.Inception_V3_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Inception_V3_Weights.verify(weights)\n",
    "\n",
    "    original_aux_logits = kwargs.get(\"aux_logits\", True)\n",
    "    if weights is not None:\n",
    "        if \"transform_input\" not in kwargs:\n",
    "            _ovewrite_named_param(kwargs, \"transform_input\", True)\n",
    "        _ovewrite_named_param(kwargs, \"aux_logits\", True)\n",
    "        _ovewrite_named_param(kwargs, \"init_weights\", False)\n",
    "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = Inception3(**kwargs)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress))\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            model.AuxLogits = None\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# The dictionary below is internal implementation detail and will be removed in v0.15\n",
    "from torchvision.models._utils import _ModelURLs\n",
    "\n",
    "\n",
    "model_urls = _ModelURLs(\n",
    "    {\n",
    "        # Inception v3 ported from TensorFlow\n",
    "        \"inception_v3_google\": Inception_V3_Weights.IMAGENET1K_V1.url,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642c9911-96a5-4dab-bff6-6c1d09db9ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=152, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=2048, out_features=152, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = batchSize()\n",
    "newmodel = Inception3(init_weights=True, transform_input=[batch_size,3,299,299])\n",
    "newmodel.eval()\n",
    "#torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "#newmodel.fc = torch.nn.Linear(in_features=2048,out_features=152)\n",
    "#print(newmodel)\n",
    "#model.features[branch_pool] =  torch.nn.AdaptiveAvgPool2d(512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0b0aed-2bd7-4a2a-81d7-336bd3ce42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(newmodel.15.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992af94f-95ef-4ffd-b0db-cdc5c33073c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import soundfile as sf\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "#%load_ext lab_black\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2200bdc9-364c-4159-b967-620afe002504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb46755f-6be2-4909-8ddb-daa2695d918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ee9695-4279-42aa-87d2-8ac3884c4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'BirdCLEF2022/train_audio/afrsil1/XC125458.ogg'\n",
    "#filename_og, sample_r = torchaudio.load('/home/coglab/miniconda3/etc/BirdCLEF2022/train_audio/afrsil1/XC125458.ogg')\n",
    "#rel_name = 'afrsil1'\n",
    "\n",
    "\n",
    "#filename = torch.Tensor.numpy(filename_og)\n",
    "#filename = filename[:1,:160000]\n",
    "#waveform, sample_rate\n",
    "#print_stats(waveform, sample_rate=sample_rate)\n",
    "#plot_waveform(waveform, sample_rate)\n",
    "#plot_specgram(waveform, sample_rate)\n",
    "#play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60eb97c2-110f-4aa3-b1f5-355eeabd384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel_spectrogram(audio_file, **spec_params):\n",
    "    sr, hop_length, n_fft, n_mels, fmin, fmax = [\n",
    "        spec_params[k] for k in [\"sr\", \"hop_length\", \"n_fft\", \"n_mels\", \"fmin\", \"fmax\"]\n",
    "    ]\n",
    "    audio, _ = librosa.core.load(audio_file, sr=sr, mono=True)\n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        audio,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        power=1,\n",
    "    )\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def pcen_bird(melspec, **spec_params):\n",
    "    \"\"\"\n",
    "    parameters are taken from [1]:\n",
    "        - [1] Lostanlen, et. al. Per-Channel Energy Normalization: Why and How. IEEE Signal Processing Letters, 26(1), 39-43.\n",
    "    \"\"\"\n",
    "    sr, hop_length = [spec_params[k] for k in [\"sr\", \"hop_length\"]]\n",
    "    return librosa.pcen(\n",
    "        melspec * (2 ** 31),\n",
    "        time_constant=0.06,\n",
    "        eps=1e-6,\n",
    "        gain=0.8,\n",
    "        power=0.25,\n",
    "        bias=10,\n",
    "        sr=sr,\n",
    "        hop_length=hop_length,\n",
    "    )\n",
    "\n",
    "\n",
    "def mel2audio(melspec, **spec_params):\n",
    "    n_fft, sr, hop_length = [spec_params[k] for k in [\"n_fft\", \"sr\", \"hop_length\"]]\n",
    "    return librosa.feature.inverse.mel_to_audio(\n",
    "        melspec, sr=sr, n_fft=n_fft, hop_length=hop_length, power=1\n",
    "    )\n",
    "\n",
    "\n",
    "def get_fullpath(filename, audio_path=\"/home/coglab/miniconda3/etc/BirdCLEF2022/train_audio/\"):\n",
    "    return f\"{audio_path}/{filename}\"\n",
    "\n",
    "\n",
    "def play_audio(audio_file):\n",
    "    display(ipd.Audio(audio_file))\n",
    "\n",
    "\n",
    "def gen_spec_and_audio(audio_name):\n",
    "    out_file = audio_name.replace(\"/\", \"_\")[:-4] + \".wav\"\n",
    "    img = plot_spectrograms(audio_name)\n",
    "    ##print(\"source audio:\")\n",
    "    play_audio(get_fullpath(audio_name))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b66fdc69-1727-4df5-bb79-229cb17a303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrograms(\n",
    "    audio_name,\n",
    "    spec_params=dict(\n",
    "        sr=32_000, hop_length=320, n_fft=1024, n_mels=128, fmin=20, fmax=14_000\n",
    "    ),\n",
    "):\n",
    "    sr, hop_length, fmin, fmax, n_mels = [\n",
    "        spec_params[k] for k in [\"sr\", \"hop_length\", \"fmin\", \"fmax\", \"n_mels\"]\n",
    "    ]\n",
    "    #############print(f\"parameters: {spec_params}\")\n",
    "    audio_file = get_fullpath(audio_name)\n",
    "    if not os.path.isfile(audio_file):\n",
    "        raise FileNotFoundError\n",
    "    melspec = create_mel_spectrogram(audio_file, **spec_params)\n",
    "    log_melspec = librosa.amplitude_to_db(melspec, ref=np.max)\n",
    "    pcen_melspec = pcen_bird(melspec, **spec_params)\n",
    "\n",
    "    return pcen_melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "280b203c-2582-4318-ac42-7a0c61494c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, is_inception=True):\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    loopcount = 0\n",
    "    \n",
    "    since = time.time()\n",
    "    batch = batchSize()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in (dataloaders): #dataloaders[phase]\n",
    "                #print(\"full set\", inputs.shape)\n",
    "                breaker = False\n",
    "                loopcount +=1\n",
    "                print(\"loop number :\",loopcount)\n",
    "                \n",
    "                if  inputs.shape != torch.Size([batch, 3, 299, 299]):\n",
    "                    print(\"Epoc end, batch smaller \", inputs.shape)\n",
    "                    breaker = True\n",
    "                    break\n",
    "                for x in inputs:\n",
    "                    if x.shape != torch.Size([3, 299, 299]):\n",
    "                        print(\"Epoc end, missing shape \", x.shape)\n",
    "                        breaker = True\n",
    "                        break\n",
    "                if breaker != False:\n",
    "                    break\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        #print(inputs.size())\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        #outs = torch.tensor([4,152], dtype=torch.float)\n",
    "                        #outs = outs.to(device)\n",
    "                        outputs, aux_outputs = model(inputs)#outs,outs#model(inputs)#torchvision.models.inception_v3(inputs) #model(inputs)#152,152\n",
    "                        #print(labels)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len((dataloaders).dataset) #dataloaders[phase]\n",
    "            epoch_acc = running_corrects.double() / len((dataloaders).dataset) #dataloaders[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4ce4e8-a17c-4bf1-8235-60e9b58764b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('miniconda3/etc/BirdCLEF2022/train_metadata.csv', low_memory=False)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# print(df.size)\n",
    "\n",
    "# # Let's say we want to split the data in 80:10:10 for train:valid:test dataset\n",
    "# train_size=0.8\n",
    "\n",
    "# X = df.drop(columns = ['primary_label']).copy()\n",
    "# y = df['primary_label']\n",
    "\n",
    "# # In the first step we will split the data in training and remaining dataset\n",
    "# X_train, X_rem, y_train, y_rem = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "# # Now since we want the valid and test size to be equal (10% each of overall data). \n",
    "# # we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "# test_size = 0.5\n",
    "# X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "# print(X_train.shape), print(y_train.shape)\n",
    "# print(X_valid.shape), print(y_valid.shape)\n",
    "# print(X_test.shape), print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caa22566-544a-4a3f-a754-20988c2f5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"/home/coglab/miniconda3/etc/BirdCLEF2022/train_audio/\"\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"inception\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 152\n",
    "\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93180dd1-b261-4ee0-b33f-dbb67598170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "csv_file = '/home/coglab/miniconda3/etc/BirdCLEF2022/train_metadata.csv'\n",
    "dest = '/home/coglab/miniconda3/etc/BirdCLEF2022/train_use.csv'\n",
    "le = preprocessing.LabelEncoder()\n",
    "our_csv = pd.read_csv(csv_file)\n",
    "le.fit(our_csv.primary_label)\n",
    "le.classes_\n",
    "our_csv['primary_label'] = le.transform(our_csv.primary_label) \n",
    "our_csv.to_csv(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592db78a-e4f6-466b-aaac-a1d5c119ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BirdsDataset(Dataset):\n",
    "#     def __init__(self, csv_file, root_dir, transform=None):\n",
    "#         self.annotations = pd.read_csv(csv_file)\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.annotations)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         path = self.annotations.iloc[index,13]\n",
    "#         print(path)\n",
    "#         img = plot_spectrograms(path)\n",
    "#         img3 = cv2.cvtColor(np.float32(img), cv2.COLOR_GRAY2RGB) \n",
    "#         formatted = (img3 * 255 / np.max(img3)).astype('uint8')\n",
    "#         PIL_image = Image.fromarray(formatted).convert('RGB')\n",
    "#         #image = PIL_image\n",
    "#         input_image = PIL_image\n",
    "        \n",
    "        \n",
    "#         preprocess = transforms.Compose([\n",
    "#                transforms.Resize(299),\n",
    "#                transforms.CenterCrop(299),\n",
    "#                transforms.ToTensor(),\n",
    "#                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#            ])\n",
    "#         image = preprocess(input_image)\n",
    "#         #print(type(image))\n",
    "#         #input_tensor = preprocess(input_image)\n",
    "#         #image_big = input_tensor.unsqueeze(0)\n",
    "#         #image = torch.squeeze(image_big)\n",
    "#         y_label = torch.tensor(int(self.annotations.iloc[index, 1]),dtype = torch.float)\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(img)\n",
    "                \n",
    "#         #print(image.size())\n",
    "#         #print(y_label.size())\n",
    "#         #print(y_label)\n",
    "#         return (image, y_label)\n",
    "\n",
    "\n",
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # # Hyperparameters\n",
    "# # in_channel = 3\n",
    "# # num_classes = 2\n",
    "# learning_rate = 3e-4\n",
    "# batch_size = batchSize()\n",
    "# # num_epochs = 10\n",
    "\n",
    "# # Load Data\n",
    "# dataset = BirdsDataset(\n",
    "#     csv_file=\"/home/coglab/miniconda3/etc/BirdCLEF2022/train_use.csv\", #train_metadata.csv\"\n",
    "#     root_dir=\"/home/coglab/miniconda3/etc/BirdCLEF2022/train_audio\",\n",
    "#     #transform=transforms.ToTensor(),\n",
    "# )\n",
    "\n",
    "# # Dataset is actually a lot larger ~25k images, just took out 10 pictures\n",
    "# # to upload to Github. It's enough to understand the structure and scale\n",
    "# # if you got more images.\n",
    "\n",
    "# proportions = [.010, .090]\n",
    "# lengths = [int(p * len(dataset)) for p in proportions]\n",
    "# lengths[-1] = len(dataset) - sum(lengths[:-1])\n",
    "# #tr_dataset, vl_dataset, = random_split(dataset, lengths)\n",
    "\n",
    "\n",
    "# train_set, test_set = torch.utils.data.random_split(dataset, lengths) # train 80% 154460 valid 20% 38616\n",
    "# train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Model\n",
    "# #model = torchvision.models.googlenet(weights=\"DEFAULT\")\n",
    "\n",
    "# ############################ freeze all layers, change final linear layer with num_classes\n",
    "# for param in newmodel.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# ########################## final layer is not frozen\n",
    "# newmodel.fc = nn.Linear(in_features=2048, out_features=num_classes)\n",
    "# newmodel.to(device)\n",
    "\n",
    "# # Loss and optimizer\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(newmodel.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1375056-967e-4947-a4ae-602c16896276",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBirdsDataset_betterTrain\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file, root_dir, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class BirdsDataset_betterTrain(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.annotations.iloc[index,13]\n",
    "        print(path)\n",
    "        img = plot_spectrograms(path)\n",
    "        \n",
    "        \n",
    "        maximum_start = len(img[0])       #3 second block\n",
    "        if maximum_start>300:\n",
    "            maximum_start = maximum_start-300\n",
    "            #print(maximum_start)\n",
    "            s = random.randint(0,maximum_start)   \n",
    "            img = img[:,s:s+300]                  #3 second block end\n",
    "            #maximum_start = len(img[0])\n",
    "\n",
    "        \n",
    "        img3 = cv2.cvtColor(np.float32(img), cv2.COLOR_GRAY2RGB) \n",
    "        formatted = (img3 * 255 / np.max(img3)).astype('uint8')\n",
    "        PIL_image = Image.fromarray(formatted).convert('RGB')\n",
    "\n",
    "        #path_img = \"/home/coglab/miniconda3/etc/images_pcen/\" + path[-11:-4] + \".jpg\" \n",
    "        #PIL_image.save(path_img)#saving imagesimg\n",
    "        \n",
    "        #image = PIL_image\n",
    "        input_image = PIL_image\n",
    "        \n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "               transforms.Resize(299),\n",
    "               transforms.CenterCrop(299),\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "           ])\n",
    "        image = preprocess(input_image)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]),dtype = torch.float)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(img)\n",
    "                \n",
    "        #print(image.size())\n",
    "        #print(y_label.size())\n",
    "        #print(y_label)\n",
    "        return (image, y_label)\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#(\"cpu\")#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Hyperparameters\n",
    "# in_channel = 3\n",
    "# num_classes = 2\n",
    "learning_rate = 3e-4\n",
    "batch_size = batchSize()\n",
    "# num_epochs = 10\n",
    "\n",
    "# Load Data\n",
    "dataset = BirdsDataset_betterTrain(\n",
    "    csv_file=\"/home/coglab/miniconda3/etc/BirdCLEF2022/train_use.csv\", #train_metadata.csv\"\n",
    "    root_dir=\"/home/coglab/miniconda3/etc/BirdCLEF2022/train_audio\",\n",
    "    #transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Dataset is actually a lot larger ~25k images, just took out 10 pictures\n",
    "# to upload to Github. It's enough to understand the structure and scale\n",
    "# if you got more images.\n",
    "\n",
    "proportions = [.8, .2]\n",
    "lengths = [int(p * len(dataset)) for p in proportions]\n",
    "lengths[-1] = len(dataset) - sum(lengths[:-1])\n",
    "#tr_dataset, vl_dataset, = random_split(dataset, lengths)\n",
    "\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, lengths) # train 80% 154460 valid 20% 38616\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model\n",
    "#model = torchvision.models.googlenet(weights=\"DEFAULT\")\n",
    "\n",
    "############################ freeze all layers, change final linear layer with num_classes\n",
    "for param in newmodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "########################## final layer is not frozen\n",
    "newmodel.fc = nn.Linear(in_features=2048, out_features=num_classes)\n",
    "newmodel.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(newmodel.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5390f81a-172d-423c-8679-b596c301c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(299),\n",
    "#     transforms.CenterCrop(299),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# trainset = torchvision.datasets(root=data_dir, train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root=data_dir, train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "#batch_size = 1\n",
    "\n",
    "classes = 'afrsil1 akekee akepa1 akiapo akikik amewig aniani apapan arcter \\\n",
    "                      barpet bcnher belkin1 bkbplo bknsti bkwpet blkfra blknod bongul \\\n",
    "                      brant brnboo brnnod brnowl brtcur bubsan buffle bulpet burpar buwtea \\\n",
    "                      cacgoo1 calqua cangoo canvas caster1 categr chbsan chemun chukar cintea \\\n",
    "                      comgal1 commyn compea comsan comwax coopet crehon dunlin elepai ercfra eurwig \\\n",
    "                      fragul gadwal gamqua glwgul gnwtea golphe grbher3 grefri gresca gryfra gwfgoo \\\n",
    "                      hawama hawcoo hawcre hawgoo hawhaw hawpet1 hoomer houfin houspa hudgod iiwi incter1 \\\n",
    "                      jabwar japqua kalphe kauama laugul layalb lcspet leasan leater1 lessca lesyel lobdow lotjae \\\n",
    "                      madpet magpet1 mallar3 masboo mauala maupar merlin mitpar moudov norcar norhar2 normoc norpin \\\n",
    "                      norsho nutman oahama omao osprey pagplo palila parjae pecsan peflov perfal pibgre pomjae puaioh \\\n",
    "                      reccar redava redjun redpha1 refboo rempar rettro ribgul rinduc rinphe rocpig rorpar rudtur ruff \\\n",
    "                      saffin sander semplo sheowl shtsan skylar snogoo sooshe sooter1 sopsku1 sora spodov sposan \\\n",
    "                      towsol wantat1 warwhe1 wesmea wessan wetshe whfibi whiter whttro wiltur yebcar yefcan zebdov'.split()\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "327f01c5-2754-44b6-8f78-7f6e23cfe841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = plot_spectrograms(\"wesmea/XC452089.ogg\")\n",
    " \n",
    "# img = plot_spectrograms(\"wesmea/XC452089.ogg\")\n",
    "# img3 = cv2.cvtColor(np.float32(img), cv2.COLOR_GRAY2RGB) \n",
    "# formatted = (img3 * 255 / np.max(img3)).astype('uint8')\n",
    "# PIL_image = Image.fromarray(formatted).convert('RGB')\n",
    "# #image = PIL_image\n",
    "# input_image = PIL_image\n",
    "        \n",
    "        \n",
    "# preprocess = transforms.Compose([\n",
    "#         transforms.Resize(299),\n",
    "#         transforms.CenterCrop(299),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     ])\n",
    "# image = preprocess(input_image)\n",
    "# inputs = image\n",
    "# outputs, aux_outputs = newmodel(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1ce4204-a2c1-4490-9709-7b7c9cf2a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#newmodel(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "009f0c67-4df4-4480-8e75-b01b6373a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "----------\n",
      "osprey/XC466204.ogg\n",
      "chukar/XC505423.ogg\n",
      "norcar/XC184580.ogg\n",
      "comsan/XC556785.ogg\n",
      "commyn/XC294325.ogg\n",
      "rinphe/XC422829.ogg\n",
      "bcnher/XC235426.ogg\n",
      "sposan/XC498129.ogg\n",
      "rinduc/XC143031.ogg\n",
      "norcar/XC142500.ogg\n",
      "norcar/XC336998.ogg\n",
      "rorpar/XC541890.ogg\n",
      "skylar/XC594472.ogg\n",
      "gwfgoo/XC595870.ogg\n",
      "lobdow/XC161342.ogg\n",
      "bcnher/XC642133.ogg\n",
      "wesmea/XC459756.ogg\n",
      "pibgre/XC596469.ogg\n",
      "hawcre/XC122132.ogg\n",
      "normoc/XC321905.ogg\n",
      "afrsil1/XC317039.ogg\n",
      "sora/XC174359.ogg\n",
      "comsan/XC569699.ogg\n",
      "norsho/XC645672.ogg\n",
      "houfin/XC465584.ogg\n",
      "canvas/XC169220.ogg\n",
      "skylar/XC182197.ogg\n",
      "skylar/XC638042.ogg\n",
      "comsan/XC484232.ogg\n",
      "redjun/XC447977.ogg\n",
      "eurwig/XC362197.ogg\n",
      "cangoo/XC378732.ogg\n",
      "mallar3/XC634632.ogg\n",
      "wiltur/XC365158.ogg\n",
      "gwfgoo/XC465612.ogg\n",
      "bcnher/XC587880.ogg\n",
      "skylar/XC319034.ogg\n",
      "comwax/XC202499.ogg\n",
      "mallar3/XC443150.ogg\n",
      "snogoo/XC143622.ogg\n",
      "comsan/XC650270.ogg\n",
      "dunlin/XC526646.ogg\n",
      "gamqua/XC636288.ogg\n",
      "bcnher/XC361996.ogg\n",
      "cangoo/XC177509.ogg\n",
      "lobdow/XC161341.ogg\n",
      "omao/XC175493.ogg\n",
      "hoomer/XC372597.ogg\n",
      "arcter/XC660413.ogg\n",
      "saffin/XC541904.ogg\n",
      "comwax/XC487379.ogg\n",
      "norcar/XC143801.ogg\n",
      "normoc/XC351829.ogg\n",
      "commyn/XC281918.ogg\n",
      "compea/XC215374.ogg\n",
      "refboo/XC207435.ogg\n",
      "brant/XC540716.ogg\n",
      "brnowl/XC580738.ogg\n",
      "rocpig/XC663912.ogg\n",
      "cangoo/XC586547.ogg\n",
      "lesyel/XC589936.ogg\n",
      "comgal1/XC296662.ogg\n",
      "redjun/XC467876.ogg\n",
      "akiapo/XC27374.ogg\n",
      "loop number : 1\n",
      "perfal/XC563847.ogg\n",
      "comsan/XC587730.ogg\n",
      "brnnod/XC110548.ogg\n",
      "whfibi/XC576253.ogg\n",
      "comwax/XC456709.ogg\n",
      "rudtur/XC546676.ogg\n",
      "eurwig/XC424353.ogg\n",
      "bcnher/XC585654.ogg\n",
      "commyn/XC40087.ogg\n",
      "cangoo/XC488801.ogg\n",
      "dunlin/XC612722.ogg\n",
      "norhar2/XC326879.ogg\n",
      "bknsti/XC375669.ogg\n",
      "skylar/XC462158.ogg\n",
      "saffin/XC50970.ogg\n",
      "norcar/XC324411.ogg\n",
      "pibgre/XC109909.ogg\n",
      "comsan/XC586746.ogg\n",
      "sora/XC311877.ogg\n",
      "dunlin/XC646763.ogg\n",
      "brant/XC163340.ogg\n",
      "cangoo/XC495554.ogg\n",
      "parjae/XC479319.ogg\n",
      "normoc/XC196664.ogg\n",
      "gwfgoo/XC607633.ogg\n",
      "cangoo/XC511453.ogg\n",
      "osprey/XC169637.ogg\n",
      "wesmea/XC543671.ogg\n",
      "ribgul/XC457113.ogg\n",
      "leater1/XC237731.ogg\n",
      "dunlin/XC666162.ogg\n",
      "mallar3/XC554058.ogg\n",
      "bcnher/XC544188.ogg\n",
      "saffin/XC537298.ogg\n",
      "saffin/XC344621.ogg\n",
      "gryfra/XC197827.ogg\n",
      "eurwig/XC528485.ogg\n",
      "norcar/XC325692.ogg\n",
      "buwtea/XC173312.ogg\n",
      "dunlin/XC526116.ogg\n",
      "gwfgoo/XC528312.ogg\n",
      "eurwig/XC266806.ogg\n",
      "rettro/XC439010.ogg\n",
      "normoc/XC553299.ogg\n",
      "skylar/XC530609.ogg\n",
      "norcar/XC463767.ogg\n",
      "blkfra/XC362925.ogg\n",
      "sora/XC278146.ogg\n",
      "houspa/XC345971.ogg\n",
      "brant/XC163344.ogg\n",
      "brnowl/XC138038.ogg\n",
      "cangoo/XC310621.ogg\n",
      "belkin1/XC624472.ogg\n",
      "caster1/XC661371.ogg\n",
      "bkbplo/XC444674.ogg\n",
      "snogoo/XC458802.ogg\n",
      "normoc/XC179351.ogg\n",
      "caster1/XC173581.ogg\n",
      "gnwtea/XC356387.ogg\n",
      "comsan/XC594516.ogg\n",
      "osprey/XC383810.ogg\n",
      "perfal/XC533792.ogg\n",
      "brnowl/XC568471.ogg\n",
      "pecsan/XC239619.ogg\n",
      "loop number : 2\n",
      "bknsti/XC145752.ogg\n",
      "gadwal/XC463290.ogg\n",
      "lobdow/XC396637.ogg\n",
      "skylar/XC548093.ogg\n",
      "spodov/XC345159.ogg\n",
      "pibgre/XC452098.ogg\n",
      "dunlin/XC439998.ogg\n",
      "warwhe1/XC27315.ogg\n",
      "gnwtea/XC636125.ogg\n",
      "lotjae/XC203619.ogg\n",
      "rinphe/XC551265.ogg\n",
      "belkin1/XC167900.ogg\n",
      "normoc/XC122159.ogg\n",
      "moudov/XC381087.ogg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m newmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m bird_model, hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepocCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m path_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/coglab/miniconda3/etc/BirdCLEF2022/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(bird_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/coglab/miniconda3/etc/BirdCLEF2022/bird_weights.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [12], line 28\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[1;32m     25\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Iterate over data.\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m (dataloaders): \u001b[38;5;66;03m#dataloaders[phase]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#print(\"full set\", inputs.shape)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     breaker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     loopcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/torch/utils/data/dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn [54], line 13\u001b[0m, in \u001b[0;36mBirdsDataset_betterTrain.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39miloc[index,\u001b[38;5;241m13\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[0;32m---> 13\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mplot_spectrograms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m maximum_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(img[\u001b[38;5;241m0\u001b[39m])       \u001b[38;5;66;03m#3 second block\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maximum_start\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m300\u001b[39m:\n",
      "Cell \u001b[0;32mIn [11], line 14\u001b[0m, in \u001b[0;36mplot_spectrograms\u001b[0;34m(audio_name, spec_params)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(audio_file):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m melspec \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mspec_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m log_melspec \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mamplitude_to_db(melspec, ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m     16\u001b[0m pcen_melspec \u001b[38;5;241m=\u001b[39m pcen_bird(melspec, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mspec_params)\n",
      "Cell \u001b[0;32mIn [10], line 5\u001b[0m, in \u001b[0;36mcreate_mel_spectrogram\u001b[0;34m(audio_file, **spec_params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_mel_spectrogram\u001b[39m(audio_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mspec_params):\n\u001b[1;32m      2\u001b[0m     sr, hop_length, n_fft, n_mels, fmin, fmax \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m         spec_params[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhop_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_fft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_mels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m     ]\n\u001b[0;32m----> 5\u001b[0m     audio, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     melspec \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(\n\u001b[1;32m      7\u001b[0m         audio,\n\u001b[1;32m      8\u001b[0m         sr\u001b[38;5;241m=\u001b[39msr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         power\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Final cleanup for dtype and contiguity\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mono:\n\u001b[0;32m--> 176\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mto_mono\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     y \u001b[38;5;241m=\u001b[39m resample(y, orig_sr\u001b[38;5;241m=\u001b[39msr_native, target_sr\u001b[38;5;241m=\u001b[39msr, res_type\u001b[38;5;241m=\u001b[39mres_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/librosa/core/audio.py:496\u001b[0m, in \u001b[0;36mto_mono\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"Convert an audio signal to mono by averaging samples across channels.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m \n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Validate the buffer.  Stereo is ok here.\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    499\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/librosa/util/decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[1;32m     94\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/cnn/lib/python3.9/site-packages/librosa/util/utils.py:293\u001b[0m, in \u001b[0;36mvalid_audio\u001b[0;34m(y, mono)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mono \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape for monophonic audio: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim=\u001b[39m\u001b[38;5;132;01m{:d}\u001b[39;00m\u001b[38;5;124m, shape=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y\u001b[38;5;241m.\u001b[39mndim, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio buffer is not finite everywhere\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#newmodel(inputs) = [batch_size,3,299,299]\n",
    "#output, aux_output = model(input)\n",
    "since = time.time()\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "newmodel.to(device)\n",
    "bird_model, hist = train_model(newmodel, train_loader, criterion, optimizer, num_epochs=epocCount(), is_inception=True)\n",
    "\n",
    "path_weight = '/home/coglab/miniconda3/etc/BirdCLEF2022/'\n",
    "\n",
    "torch.save(bird_model.state_dict(), '/home/coglab/miniconda3/etc/BirdCLEF2022/bird_weights.pt')\n",
    "torch.save(bird_model, '/home/coglab/miniconda3/etc/BirdCLEF2022/bird_model.pt')\n",
    "\n",
    "print(\"model saved. \", path_weight)\n",
    "\n",
    "torch.save(hist,'/home/coglab/miniconda3/etc/BirdCLEF2022/bird_model_logm.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eeb9ce3b-9f5a-493c-b2d8-7deda746a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.image as mpimg\n",
    "#image = mpimg.imread(\"chelsea-the-cat.png\")\n",
    "\n",
    "#class BirdsDataset(Dataset):\n",
    "#    def __init__(self, csv_file, root_dir, transform=None):\n",
    "##        self.annotations = pd.read_csv(csv_file)\n",
    "#        self.root_dir = root_dir\n",
    "#        self.transform = transform\n",
    "\n",
    "# df = our_csv#pd.read_csv(\"/home/coglab/miniconda3/etc/BirdCLEF2022/train_use.csv\"), #train_metadata.csv\"\n",
    "# root_dir=\"/home/coglab/miniconda3/etc/BirdCLEF2022/train_audio\",\n",
    "\n",
    "# path = our_csv.iloc[index,13]\n",
    "# print(path)\n",
    "# img = plot_spectrograms(path)\n",
    "# img3 = cv2.cvtColor(np.float32(img), cv2.COLOR_GRAY2RGB) \n",
    "# formatted = (img3 * 255 / np.max(img3)).astype('uint8')\n",
    "# PIL_image = Image.fromarray(formatted).convert('RGB')\n",
    "#         #image = PIL_image\n",
    "# input_image = PIL_image\n",
    "        \n",
    "        \n",
    "# preprocess = transforms.Compose([\n",
    "#                transforms.Resize(299),\n",
    "#                transforms.CenterCrop(299),\n",
    "#                transforms.ToTensor(),\n",
    "#                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#            ])\n",
    "# image = preprocess(input_image)\n",
    "#         #print(type(image))\n",
    "#         #input_tensor = preprocess(input_image)\n",
    "#         #image_big = input_tensor.unsqueeze(0)\n",
    "#         #image = torch.squeeze(image_big)\n",
    "# y_label = torch.tensor(int(self.annotations.iloc[index, 0]),dtype = torch.float)\n",
    "\n",
    "# if self.transform:\n",
    "#     image = self.transform(img)\n",
    "                \n",
    "#         #print(image.size())\n",
    "#         #print(y_label.size())\n",
    "# print(y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4aa7867d-ac87-494b-b023-e856d06794fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1790)\n"
     ]
    }
   ],
   "source": [
    "# img = plot_spectrograms('/brnowl/XC138613.ogg')\n",
    "# img = np.array(img)\n",
    "# img2 = np.float32(img)\n",
    "# img3 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB) \n",
    "# PIL_image = Image.fromarray(img3)\n",
    "# img.pcen_melspec\n",
    "# img3 = cv2.cvtColor(np.float32(img), cv2.COLOR_GRAY2RGB) \n",
    "# print(img3)\n",
    "\n",
    "img = plot_spectrograms('brnowl/XC138613.ogg')\n",
    "print(img.shape)\n",
    "\n",
    "# img3 = cv2.cvtColor(np.float32(img), cv2.COLOR_GRAY2RGB) \n",
    "# formatted = (img3 * 255 / np.max(img3)).astype('uint8')\n",
    "# PIL_image = Image.fromarray(formatted).convert('RGB')\n",
    "# print(PIL_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aea38d83-d42b-4e0e-8ed7-fe31c6517d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIL_image = Image.fromarray(np.uint8(img3)).convert('RGB')\n",
    "#print(img.size)\n",
    "#PIL_image = Image.fromarray(img3.astype('uint8'),  mode=\"RGB\")\n",
    "#PIL_image.sh\n",
    "#cv2.imshow(\"kuva\",img3)\n",
    "\n",
    "# img3 = cv2.cvtColor(np.float32(img), cv2.COLOR_GRAY2RGB) \n",
    "# formatted = (img3 * 255 / np.max(img3)).astype('uint8')\n",
    "# PIL_image = Image.fromarray(formatted).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f648834-a617-454a-8179-696593b49573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "# The thing here thats troubling is that I dont see the connection from the PCEN \n",
    "# to the CNN\n",
    "\n",
    "#TODO: copy the colab sample and go try use the audio as an input :thinking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a373c28e-990c-4f55-ae0c-2f13d818f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample execution (requires torchvision)\n",
    "# from torchvision import transforms\n",
    "# input_image = PIL_image\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize(299),\n",
    "#     transforms.CenterCrop(299),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# input_tensor = preprocess(input_image)\n",
    "# input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# # move the input and model to GPU for speed if available\n",
    "# if torch.cuda.is_available():\n",
    "#     input_batch = input_batch.to('cuda')\n",
    "#     model.to('cuda')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   output = model(input_batch)\n",
    "# # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "# #print(output[0])\n",
    "# # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# #print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "420e67b2-4522-435c-801b-890308c0553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ImageNet labels\n",
    "#!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41567251-7012-498b-ace9-7f4f54344525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the categories\n",
    "# with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "#     categories = [s.strip() for s in f.readlines()]\n",
    "# # Show top categories per image\n",
    "# top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "# for i in range(top5_prob.size(0)):\n",
    "#     print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4deff128-a6b4-48b4-8dec-311807bc33c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5033950805664062e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "since = time.time()\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print(time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64e3258c-fb03-467f-9f78-93e2bd0ee2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2472,)\n",
      "2172\n",
      "1296 1596 2172\n",
      "(128, 2472)\n",
      "(128, 300)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "img = plot_spectrograms('brant/XC588758.ogg')\n",
    "print(img[1].shape)\n",
    "maximum_start = len(img[0])\n",
    "maximum_start = maximum_start-300\n",
    "print(maximum_start)\n",
    "s = random.randint(1,maximum_start)\n",
    "print(s, s+300, maximum_start)\n",
    "print(img.shape)\n",
    "img = img[:,s:s+300]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d8e60-561d-45da-b109-2d93bc9c0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maximum_start = len(img[0])-300\n",
    "s = random.randint(0,maximum_start)\n",
    "img = img[:,s:s+300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1591d-004a-462a-9238-bcbb79ed292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723be28c-f4ea-43b4-9b9f-c0fd34c080bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img[:,0:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c262326-c1a6-4614-9fb0-eb079142cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = torch.load('miniconda3/etc/BirdCLEF2022/bird_model.pt')\n",
    "\n",
    "#print(\"Model's state_dict:\")\n",
    "#for param_tensor in H.state_dict():\n",
    "#    print(param_tensor, \"\\t\", H.state_dict()[param_tensor].size())\n",
    "\n",
    "#print()\n",
    "\n",
    "\n",
    "#H = H.load_state_dict()#(state_dic, strict=False)\n",
    "#load_state_dict(sd, strict=False)\n",
    "\n",
    "#hist\n",
    "hist = Tensor.cpu(hist) \n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "#plt.plot(np.arange(0, len(H.history[\"loss\"])), H.history[\"loss\"], label=\"train_loss\")\n",
    "#plt.plot(np.arange(0, len(H.history[\"val_loss\"])), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "#plt.plot(np.arange(0, len(H.history[\"accuracy\"])), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, len(hist)), hist, label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1e5f0-591e-4fa9-b2bc-2bd895cfcf84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
