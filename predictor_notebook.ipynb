{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ee5d2f-e45a-45b8-8b1c-b924af0c0e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./archive/pytorch-image-models-master\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /home/coglab/miniconda3/envs/birdclef/lib/python3.9/site-packages (from timm==0.6.5) (1.9.1)\n",
      "Requirement already satisfied: torchvision in /home/coglab/miniconda3/envs/birdclef/lib/python3.9/site-packages (from timm==0.6.5) (0.10.1)\n",
      "Requirement already satisfied: typing_extensions in /home/coglab/miniconda3/envs/birdclef/lib/python3.9/site-packages (from torch>=1.4->timm==0.6.5) (4.5.0)\n",
      "Requirement already satisfied: numpy in /home/coglab/miniconda3/envs/birdclef/lib/python3.9/site-packages (from torchvision->timm==0.6.5) (1.23.5)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/coglab/miniconda3/envs/birdclef/lib/python3.9/site-packages (from torchvision->timm==0.6.5) (9.3.0)\n",
      "Building wheels for collected packages: timm\n",
      "  Building wheel for timm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for timm: filename=timm-0.6.5-py3-none-any.whl size=513111 sha256=f8d6a816b41f491ab89be4d7ff3e79c26a728b7bdf79828ea2501659a26dfc62\n",
      "  Stored in directory: /home/coglab/.cache/pip/wheels/ab/f9/04/b7284405cd90ab06be3e1eb90a9e409406bc7d134f33a2cc51\n",
      "Successfully built timm\n",
      "Installing collected packages: timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.6.5\n",
      "    Uninstalling timm-0.6.5:\n",
      "      Successfully uninstalled timm-0.6.5\n",
      "Successfully installed timm-0.6.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "!pip install /home/coglab/miniconda3/etc/archive/pytorch-image-models-master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f709f-8ebd-472e-a934-d3405f9e4ab7",
   "metadata": {},
   "source": [
    "versions should be:\n",
    "## 1.9.1+cu102\n",
    "## 0.9.1\n",
    "## 0.10.1+cu102\n",
    "## 1.5.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4c7d8b-e13d-4831-b0ce-09834733ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1\n",
      "0.9.0a0+a85b239\n",
      "0.10.1\n",
      "1.5.9\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import torch; print(torch.__version__)\"\n",
    "!python3 -c \"import torchaudio; print(torchaudio.__version__)\"\n",
    "!python3 -c \"import torchvision ; print(torchvision .__version__)\"\n",
    "!python3 -c \"import pytorch_lightning ; print(pytorch_lightning .__version__)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47008c85-a9f7-42b0-8e2c-da6797dbef1b",
   "metadata": {},
   "source": [
    "## 0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6799c32-936e-459b-b623-ae1c0ce9c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import torchvision; print(torchvision.__version__)\" #this should be 0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b315d8-effa-4cc9-97cf-37d2b8faca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762d88d4-36a3-430d-a5c0-7390f10ae40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch.utils.data as torchdata\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e91b3a6-3cc4-4268-b7f0-2673b08df92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from torchaudio.transforms import AmplitudeToDB, MelSpectrogram\n",
    "import timm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5f60b1-d883-431a-883e-5c6eef169736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchSize():\n",
    "    return 64\n",
    "def epocCount():\n",
    "    return 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d5231-1bda-4b98-8278-20cb1794096e",
   "metadata": {},
   "source": [
    "Imports ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76460b72-0910-4321-92cb-a8eb13d410c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_x = timm.create_model('inception_v3', pretrained=True, num_classes=152)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdc9da-c991-446e-86d3-5568c0de5938",
   "metadata": {},
   "source": [
    "## Write import so that it can be uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa21282d-800a-4c1f-9c54-b8ce2723064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import torchvision\n",
    "from torchvision_2.transforms._presets import ImageClassification\n",
    "from torchvision_2.utils import _log_api_usage_once\n",
    "from torchvision_2.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision_2.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision_2.models._utils import _ovewrite_named_param, handle_legacy_interface\n",
    "\n",
    "\n",
    "__all__ = [\"Inception3\", \"InceptionOutputs\", \"_InceptionOutputs\", \"Inception_V3_Weights\", \"inception_v3\"]\n",
    "\n",
    "\n",
    "InceptionOutputs = namedtuple(\"InceptionOutputs\", [\"logits\", \"aux_logits\"])\n",
    "InceptionOutputs.__annotations__ = {\"logits\": Tensor, \"aux_logits\": Optional[Tensor]}\n",
    "\n",
    "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
    "# _InceptionOutputs set here for backwards compat\n",
    "_InceptionOutputs = InceptionOutputs\n",
    "\n",
    "\n",
    "class Inception3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 152,\n",
    "        aux_logits: bool = True,\n",
    "        transform_input: bool = False,\n",
    "        inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,\n",
    "        init_weights: Optional[bool] = None,\n",
    "        dropout: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if inception_blocks is None:\n",
    "            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]\n",
    "        if init_weights is None:\n",
    "            warnings.warn(\n",
    "                \"The default weight initialization of inception_v3 will be changed in future releases of \"\n",
    "                \"torchvision. If you wish to keep the old behavior (which leads to long initialization times\"\n",
    "                \" due to scipy/scipy#11299), please set init_weights=True.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            init_weights = True\n",
    "        if len(inception_blocks) != 7:\n",
    "            raise ValueError(f\"length of inception_blocks should be 7 instead of {len(inception_blocks)}\")\n",
    "        conv_block = inception_blocks[0]\n",
    "        inception_a = inception_blocks[1]\n",
    "        inception_b = inception_blocks[2]\n",
    "        inception_c = inception_blocks[3]\n",
    "        inception_d = inception_blocks[4]\n",
    "        inception_e = inception_blocks[5]\n",
    "        inception_aux = inception_blocks[6]\n",
    "\n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n",
    "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
    "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
    "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
    "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
    "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
    "        self.Mixed_6a = inception_b(288)\n",
    "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
    "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
    "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
    "        self.AuxLogits: Optional[nn.Module] = None\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = inception_aux(768, num_classes)\n",
    "        self.Mixed_7a = inception_d(768)\n",
    "        self.Mixed_7b = inception_e(1280)\n",
    "        self.Mixed_7c = inception_e(2048)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "        if init_weights:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                    stddev = float(m.stddev) if hasattr(m, \"stddev\") else 0.1  # type: ignore\n",
    "                    torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=stddev, a=-2, b=2)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _transform_input(self, x: Tensor) -> Tensor:\n",
    "        if self.transform_input:\n",
    "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
    "        return x\n",
    "\n",
    "    def _forward(self, x: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = self.maxpool2(x)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        aux: Optional[Tensor] = None\n",
    "        if self.AuxLogits is not None:\n",
    "            if self.training:\n",
    "                aux = self.AuxLogits(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = self.avgpool(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = self.dropout(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x, aux\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, x: Tensor, aux: Optional[Tensor]) -> InceptionOutputs:\n",
    "        if self.training and self.aux_logits:\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return x  # type: ignore[return-value]\n",
    "\n",
    "    def forward(self, x: Tensor) -> InceptionOutputs:\n",
    "        x = self._transform_input(x)\n",
    "        x, aux = self._forward(x)\n",
    "        aux_defined = self.training and self.aux_logits\n",
    "        if torch.jit.is_scripting():\n",
    "            if not aux_defined:\n",
    "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
    "            return InceptionOutputs(x, aux)\n",
    "        else:\n",
    "            return self.eager_outputs(x, aux)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, pool_features: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionB(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionC(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, channels_7x7: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionD(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionE(nn.Module):\n",
    "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        outputs = self._forward(x)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, num_classes: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if conv_block is None:\n",
    "            conv_block = BasicConv2d\n",
    "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
    "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
    "        self.conv1.stddev = 0.01  # type: ignore[assignment]\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.fc.stddev = 0.001  # type: ignore[assignment]\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # N x 768 x 17 x 17\n",
    "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
    "        # N x 768 x 5 x 5\n",
    "        x = self.conv0(x)\n",
    "        # N x 128 x 5 x 5\n",
    "        x = self.conv1(x)\n",
    "        # N x 768 x 1 x 1\n",
    "        # Adaptive average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # N x 768 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 768\n",
    "        x = self.fc(x)\n",
    "        # N x 1000\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "\n",
    "class Inception_V3_Weights(WeightsEnum):\n",
    "    IMAGENET1K_V1 = Weights(\n",
    "        url=\"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\",\n",
    "        transforms=partial(ImageClassification, crop_size=299, resize_size=342),\n",
    "        meta={\n",
    "            \"num_params\": 27161264,\n",
    "            \"min_size\": (75, 75),\n",
    "            \"categories\": _IMAGENET_CATEGORIES,\n",
    "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#inception-v3\",\n",
    "            \"_metrics\": {\n",
    "                \"ImageNet-1K\": {\n",
    "                    \"acc@1\": 77.294,\n",
    "                    \"acc@5\": 93.450,\n",
    "                }\n",
    "            },\n",
    "            \"_ops\": 5.713,\n",
    "            \"_file_size\": 103.903,\n",
    "            \"_docs\": \"\"\"These weights are ported from the original paper.\"\"\",\n",
    "        },\n",
    "    )\n",
    "    DEFAULT = IMAGENET1K_V1\n",
    "\n",
    "\n",
    "#@register_model()\n",
    "#@handle_legacy_interface(weights=(\"pretrained\", Inception_V3_Weights.IMAGENET1K_V1))\n",
    "def inception_v3(*, weights: Optional[Inception_V3_Weights] = None, progress: bool = True, **kwargs: Any) -> Inception3:\n",
    "    \"\"\"\n",
    "    Inception v3 model architecture from\n",
    "    `Rethinking the Inception Architecture for Computer Vision <http://arxiv.org/abs/1512.00567>`_.\n",
    "    .. note::\n",
    "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
    "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
    "    Args:\n",
    "        weights (:class:`~torchvision.models.Inception_V3_Weights`, optional): The\n",
    "            pretrained weights for the model. See\n",
    "            :class:`~torchvision.models.Inception_V3_Weights` below for\n",
    "            more details, and possible values. By default, no pre-trained\n",
    "            weights are used.\n",
    "        progress (bool, optional): If True, displays a progress bar of the\n",
    "            download to stderr. Default is True.\n",
    "        **kwargs: parameters passed to the ``torchvision.models.Inception3``\n",
    "            base class. Please refer to the `source code\n",
    "            <https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py>`_\n",
    "            for more details about this class.\n",
    "    .. autoclass:: torchvision.models.Inception_V3_Weights\n",
    "        :members:\n",
    "    \"\"\"\n",
    "    weights = Inception_V3_Weights.verify(weights)\n",
    "\n",
    "    original_aux_logits = kwargs.get(\"aux_logits\", True)\n",
    "    if weights is not None:\n",
    "        if \"transform_input\" not in kwargs:\n",
    "            _ovewrite_named_param(kwargs, \"transform_input\", True)\n",
    "        _ovewrite_named_param(kwargs, \"aux_logits\", True)\n",
    "        _ovewrite_named_param(kwargs, \"init_weights\", False)\n",
    "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = Inception3(**kwargs)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress))\n",
    "        if not original_aux_logits:\n",
    "            model.aux_logits = False\n",
    "            model.AuxLogits = None\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# The dictionary below is internal implementation detail and will be removed in v0.15\n",
    "# from torchvision.models._utils import _ModelURLs\n",
    "\n",
    "\n",
    "# model_urls = _ModelURLs(\n",
    "#     {\n",
    "#         # Inception v3 ported from TensorFlow\n",
    "#         \"inception_v3_google\": Inception_V3_Weights.IMAGENET1K_V1.url,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51b9b0c-bd46-47c7-a340-3fc341a8acb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=152, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=2048, out_features=152, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = batchSize()\n",
    "newmodel = Inception3(init_weights=True, transform_input=[batch_size,3,299,299])\n",
    "newmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e58da68-b584-4508-9c1d-7d2cb4203127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "    \n",
    "def get_logger(out_file=None):\n",
    "    logger = logging.getLogger()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        fh = logging.FileHandler(out_file)\n",
    "        fh.setFormatter(formatter)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        logger.addHandler(fh)\n",
    "    logger.info(\"logger set up\")\n",
    "    return logger\n",
    "    \n",
    "    \n",
    "@contextmanager\n",
    "def timer(name: str, logger: Optional[logging.Logger] = None):\n",
    "    t0 = time.time()\n",
    "    msg = f\"[{name}] start\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)\n",
    "    yield\n",
    "\n",
    "    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n",
    "    if logger is None:\n",
    "        print(msg)\n",
    "    else:\n",
    "        logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a5e3759-568f-40b0-b023-ea9763d84bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 12:32:27,911 - INFO - logger set up\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(\"main.log\")\n",
    "set_seed(1213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3214b7a-13bb-40a5-a23b-7452a4477adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_num: 1\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    ######################\n",
    "    # Globals #\n",
    "    ######################\n",
    "    seed = 1213\n",
    "\n",
    "    ######################\n",
    "    # Data #\n",
    "    ######################\n",
    "    train_datadir = Path(\"../input/birdclef-2022/train_audio\")\n",
    "    train_csv = \"../input/birdclef-2022/train_metadata.csv\"\n",
    "    #train_soundscape = \"../input/birdclef-2022/train_soundscape_labels.csv\"\n",
    "\n",
    "    ######################\n",
    "    # Dataset #\n",
    "    ######################\n",
    "    transforms = {\n",
    "        \"train\": [{\"name\": \"Normalize\"}],\n",
    "        \"valid\": [{\"name\": \"Normalize\"}],\n",
    "        \"test\": [{\"name\": \"Normalize\"}]\n",
    "    }\n",
    "    period = 30\n",
    "    n_mels = 128\n",
    "    \n",
    "    sample_rate = 32000\n",
    "    target_columns = [\n",
    "        \"afrsil1\",\n",
    "        \"akekee\",\n",
    "        \"akepa1\",\n",
    "        \"akiapo\",\n",
    "        \"akikik\",\n",
    "        \"amewig\",\n",
    "        \"aniani\",\n",
    "        \"apapan\",\n",
    "        \"arcter\",\n",
    "        \"barpet\",\n",
    "        \"bcnher\",\n",
    "        \"belkin1\",\n",
    "        \"bkbplo\",\n",
    "        \"bknsti\",\n",
    "        \"bkwpet\",\n",
    "        \"blkfra\",\n",
    "        \"blknod\",\n",
    "        \"bongul\",\n",
    "        \"brant\",\n",
    "        \"brnboo\",\n",
    "        \"brnnod\",\n",
    "        \"brnowl\",\n",
    "        \"brtcur\",\n",
    "        \"bubsan\",\n",
    "        \"buffle\",\n",
    "        \"bulpet\",\n",
    "        \"burpar\",\n",
    "        \"buwtea\",\n",
    "        \"cacgoo1\",\n",
    "        \"calqua\",\n",
    "        \"cangoo\",\n",
    "        \"canvas\",\n",
    "        \"caster1\",\n",
    "        \"categr\",\n",
    "        \"chbsan\",\n",
    "        \"chemun\",\n",
    "        \"chukar\",\n",
    "        \"cintea\",\n",
    "        \"comgal1\",\n",
    "        \"commyn\",\n",
    "        \"compea\",\n",
    "        \"comsan\",\n",
    "        \"comwax\",\n",
    "        \"coopet\",\n",
    "        \"crehon\",\n",
    "        \"dunlin\",\n",
    "        \"elepai\",\n",
    "        \"ercfra\",\n",
    "        \"eurwig\",\n",
    "        \"fragul\",\n",
    "        \"gadwal\",\n",
    "        \"gamqua\",\n",
    "        \"glwgul\",\n",
    "        \"gnwtea\",\n",
    "        \"golphe\",\n",
    "        \"grbher3\",\n",
    "        \"grefri\",\n",
    "        \"gresca\",\n",
    "        \"gryfra\",\n",
    "        \"gwfgoo\",\n",
    "        \"hawama\",\n",
    "        \"hawcoo\",\n",
    "        \"hawcre\",\n",
    "        \"hawgoo\",\n",
    "        \"hawhaw\",\n",
    "        \"hawpet1\",\n",
    "        \"hoomer\",\n",
    "        \"houfin\",\n",
    "        \"houspa\",\n",
    "        \"hudgod\",\n",
    "        \"iiwi\",\n",
    "        \"incter1\",\n",
    "        \"jabwar\",\n",
    "        \"japqua\",\n",
    "        \"kalphe\",\n",
    "        \"kauama\",\n",
    "        \"laugul\",\n",
    "        \"layalb\",\n",
    "        \"lcspet\",\n",
    "        \"leasan\",\n",
    "        \"leater1\",\n",
    "        \"lessca\",\n",
    "        \"lesyel\",\n",
    "        \"lobdow\",\n",
    "        \"lotjae\",\n",
    "        \"madpet\",\n",
    "        \"magpet1\",\n",
    "        \"mallar3\",\n",
    "        \"masboo\",\n",
    "        \"mauala\",\n",
    "        \"maupar\",\n",
    "        \"merlin\",\n",
    "        \"mitpar\",\n",
    "        \"moudov\",\n",
    "        \"norcar\",\n",
    "        \"norhar2\",\n",
    "        \"normoc\",\n",
    "        \"norpin\",\n",
    "        \"norsho\",\n",
    "        \"nutman\",\n",
    "        \"oahama\",\n",
    "        \"omao\",\n",
    "        \"osprey\",\n",
    "        \"pagplo\",\n",
    "        \"palila\",\n",
    "        \"parjae\",\n",
    "        \"pecsan\",\n",
    "        \"peflov\",\n",
    "        \"perfal\",\n",
    "        \"pibgre\",\n",
    "        \"pomjae\",\n",
    "        \"puaioh\",\n",
    "        \"reccar\",\n",
    "        \"redava\",\n",
    "        \"redjun\",\n",
    "        \"redpha1\",\n",
    "        \"refboo\",\n",
    "        \"rempar\",\n",
    "        \"rettro\",\n",
    "        \"ribgul\",\n",
    "        \"rinduc\",\n",
    "        \"rinphe\",\n",
    "        \"rocpig\",\n",
    "        \"rorpar\",\n",
    "        \"rudtur\",\n",
    "        \"ruff\",\n",
    "        \"saffin\",\n",
    "        \"sander\",\n",
    "        \"semplo\",\n",
    "        \"sheowl\",\n",
    "        \"shtsan\",\n",
    "        \"skylar\",\n",
    "        \"snogoo\",\n",
    "        \"sooshe\",\n",
    "        \"sooter1\",\n",
    "        \"sopsku1\",\n",
    "        \"sora\",\n",
    "        \"spodov\",\n",
    "        \"sposan\",\n",
    "        \"towsol\",\n",
    "        \"wantat1\",\n",
    "        \"warwhe1\",\n",
    "        \"wesmea\",\n",
    "        \"wessan\",\n",
    "        \"wetshe\",\n",
    "        \"whfibi\",\n",
    "        \"whiter\",\n",
    "        \"whttro\",\n",
    "        \"wiltur\",\n",
    "        \"yebcar\",\n",
    "        \"yefcan\",\n",
    "        \"zebdov\",\n",
    "    ]\n",
    "    bird2id = {b: i for i, b in enumerate(target_columns)}\n",
    "    id2bird = {i: b for i, b in enumerate(target_columns)}\n",
    "    scored_birds = [\"akiapo\", \"aniani\", \"apapan\", \"barpet\", \"crehon\", \"elepai\", \"ercfra\", \"hawama\", \"hawcre\", \"hawgoo\", \"hawhaw\", \"hawpet1\", \"houfin\", \"iiwi\", \"jabwar\", \"maupar\", \"omao\", \"puaioh\", \"skylar\", \"warwhe1\", \"yefcan\"]\n",
    "    loader_params = {\n",
    "        \"train\": {\n",
    "            \"batch_size\": 64,\n",
    "            \"num_workers\": 20,\n",
    "            \"shuffle\": True\n",
    "        },\n",
    "        \"valid\": {\n",
    "            \"batch_size\": 64,\n",
    "            \"num_workers\": 20,\n",
    "            \"shuffle\": False\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"batch_size\": 64,\n",
    "            \"num_workers\": 20,\n",
    "            \"shuffle\": False\n",
    "        }\n",
    "    }\n",
    "    models_cfg =  [{\"inception_v3\": \"tester2.pth\" }]  \n",
    "    \n",
    "    #[{\"inception_v3\": \"/kaggle/input/birdclef22-inception-v3-pcen-weights/bird_model.pt\"}]\n",
    "    #[{\"inception_v3\": \"/kaggle/input/birdclef22-inception-v3-pcen-weights/inception_v3_google-1a9a5a14.pth\" }]  \n",
    "    #[{\"inception_v3\": \"/kaggle/input/birdclef22-inception-v3-pcen-weights/bird_model.pt\",pretrained=True}] \n",
    "    #[{\"resnest26d\": \"../input/birdclef2022-weights/resnest26d/fold0/birdclef_2022/10p941rk/checkpoints/best_f1.ckpt\",}]\n",
    "    num_classes = len(target_columns)\n",
    "print(f\"model_num: {len(CFG.models_cfg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87a76e8-507f-4ae0-93e8-4571adfbf33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'inception_v3': 'tester2.pth'}]\n"
     ]
    }
   ],
   "source": [
    "print(CFG.models_cfg)\n",
    "#models = CFG.models_cfg\n",
    "models = newmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a70362-bfa3-4a65-ba23-8884fb8d5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 32000\n",
    "#DATADIR = Path(\"../input/birdclef-2022/test_soundscapes/\") ORIGINAL\n",
    "DATADIR = Path(\"birdclef-2022/test_soundscapes/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef1d601e-0e67-418a-b2de-d86fd72c3172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soundscape_1000170626_akiapo_5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soundscape_1000170626_akiapo_10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soundscape_1000170626_akiapo_15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            row_id  target\n",
       "0   soundscape_1000170626_akiapo_5   False\n",
       "1  soundscape_1000170626_akiapo_10   False\n",
       "2  soundscape_1000170626_akiapo_15   False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_audios = list(DATADIR.glob(\"*.ogg\"))\n",
    "sample_submission = pd.read_csv('birdclef-2022/sample_submission.csv')\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27be8977-6ed4-4980-b665-713e37990669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torchdata.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip: np.ndarray, train_period=30, \n",
    "                 waveform_transforms=None):\n",
    "        self.df = df\n",
    "        self.clip = np.concatenate([clip, clip, clip])\n",
    "        self.train_period = train_period\n",
    "        self.waveform_transforms=waveform_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        SR = 32000\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "\n",
    "        end_seconds = int(sample.seconds)\n",
    "        start_seconds = int(end_seconds - 5)\n",
    "        \n",
    "        end_index = int(SR * (end_seconds + (self.train_period - 5) / 2) + len(self.clip) // 3)\n",
    "        start_index = int(SR * (start_seconds - (self.train_period - 5) / 2) + len(self.clip) // 3)\n",
    "        \n",
    "        y = self.clip[start_index:end_index].astype(np.float32)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "\n",
    "        if self.waveform_transforms:\n",
    "            y = self.waveform_transforms(y)\n",
    "\n",
    "        y = np.nan_to_num(y)\n",
    "        \n",
    "        return y, row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a99fb8-1fb5-4242-8481-b41a736bb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase: str):\n",
    "    transforms = CFG.transforms\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if globals().get(trns_name) is not None:\n",
    "                trns_cls = globals()[trns_name]\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return Compose(trns_list)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def get_waveform_transforms(config: dict, phase: str):\n",
    "    return get_transforms(config, phase)\n",
    "\n",
    "\n",
    "def get_spectrogram_transforms(config: dict, phase: str):\n",
    "    transforms = config.get('spectrogram_transforms')\n",
    "    if transforms is None:\n",
    "        return None\n",
    "    else:\n",
    "        if transforms[phase] is None:\n",
    "            return None\n",
    "        trns_list = []\n",
    "        for trns_conf in transforms[phase]:\n",
    "            trns_name = trns_conf[\"name\"]\n",
    "            trns_params = {} if trns_conf.get(\"params\") is None else \\\n",
    "                trns_conf[\"params\"]\n",
    "            if hasattr(A, trns_name):\n",
    "                trns_cls = A.__getattribute__(trns_name)\n",
    "                trns_list.append(trns_cls(**trns_params))\n",
    "            else:\n",
    "                trns_cls = globals().get(trns_name)\n",
    "                if trns_cls is not None:\n",
    "                    trns_list.append(trns_cls(**trns_params))\n",
    "\n",
    "        if len(trns_list) > 0:\n",
    "            return A.Compose(trns_list, p=1.0)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "class Normalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        max_vol = np.abs(y).max()\n",
    "        y_vol = y * 1 / max_vol\n",
    "        return np.asfortranarray(y_vol)\n",
    "\n",
    "\n",
    "class NewNormalize:\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        y_mm = y - y.mean()\n",
    "        return y_mm / y_mm.abs().max()\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class NoiseInjection(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.noise_level = (0.0, max_noise_level)\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        noise_level = np.random.uniform(*self.noise_level)\n",
    "        noise = np.random.randn(len(y))\n",
    "        augmented = (y + noise * noise_level).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class GaussianNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y ** 2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise ** 2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y ** 2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise ** 2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PitchShift(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_range=5, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_range = max_range\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        n_steps = np.random.randint(-self.max_range, self.max_range)\n",
    "        augmented = librosa.effects.pitch_shift(y, self.sr, n_steps)\n",
    "        return augmented\n",
    "\n",
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1, sr=32000):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_rate = max_rate\n",
    "        self.sr = sr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "def _db2float(db: float, amplitude=True):\n",
    "    if amplitude:\n",
    "        return 10**(db / 20)\n",
    "    else:\n",
    "        return 10 ** (db / 10)\n",
    "\n",
    "\n",
    "def volume_down(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for decreasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to decrease\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with decreased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(-db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "def volume_up(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for increasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to increase\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with increased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(db)\n",
    "    return applied\n",
    "\n",
    "class RandomVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        if db >= 0:\n",
    "            return volume_up(y, db)\n",
    "        else:\n",
    "            return volume_down(y, db)\n",
    "\n",
    "\n",
    "class OneOf:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray):\n",
    "        n_trns = len(self.transforms)\n",
    "        trns_idx = np.random.choice(n_trns)\n",
    "        trns = self.transforms[trns_idx]\n",
    "        y = trns(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class CosineVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "        dbs = _db2float(cosine * db)\n",
    "        return y * dbs\n",
    "\n",
    "\n",
    "def drop_stripes(image: np.ndarray, dim: int, drop_width: int, stripes_num: int):\n",
    "    total_width = image.shape[dim]\n",
    "    lowest_value = image.min()\n",
    "    for _ in range(stripes_num):\n",
    "        distance = np.random.randint(low=0, high=drop_width, size=(1,))[0]\n",
    "        begin = np.random.randint(\n",
    "            low=0, high=total_width - distance, size=(1,))[0]\n",
    "\n",
    "        if dim == 0:\n",
    "            image[begin:begin + distance] = lowest_value\n",
    "        elif dim == 1:\n",
    "            image[:, begin + distance] = lowest_value\n",
    "        elif dim == 2:\n",
    "            image[:, :, begin + distance] = lowest_value\n",
    "    return image\n",
    "\n",
    "class TimeFreqMasking(ImageOnlyTransform):\n",
    "    def __init__(self,\n",
    "                 time_drop_width: int,\n",
    "                 time_stripes_num: int,\n",
    "                 freq_drop_width: int,\n",
    "                 freq_stripes_num: int,\n",
    "                 always_apply=False,\n",
    "                 p=0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.time_drop_width = time_drop_width\n",
    "        self.time_stripes_num = time_stripes_num\n",
    "        self.freq_drop_width = freq_drop_width\n",
    "        self.freq_stripes_num = freq_stripes_num\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        img_ = img.copy()\n",
    "        if img.ndim == 2:\n",
    "            img_ = drop_stripes(\n",
    "                img_, dim=0, drop_width=self.freq_drop_width, stripes_num=self.freq_stripes_num)\n",
    "            img_ = drop_stripes(\n",
    "                img_, dim=1, drop_width=self.time_drop_width, stripes_num=self.time_stripes_num)\n",
    "        return img_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed0cb6-740f-4a05-80a3-71fe3e24ae10",
   "metadata": {},
   "source": [
    "## Load the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6c84d8-3749-43d6-bec7-cbeed9dfaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "\n",
    "def prepare_model_for_inference(model, path: Path):\n",
    "    if not torch.cuda.is_available():\n",
    "        ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    else:\n",
    "        ckpt = torch.load(path) #model.load_state_dict(path) #ckpt = torch.load(path) model.load(path)\n",
    "    model.load_state_dict(ckpt) # #model.load_state_dict(ckpt[\"state_dict\"]) #load_state_dict(state_dict, strict=True) model.load_state_dict(torch.load(ckpt))\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bab2653e-2606-468e-bd96-cf20e48f76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mel_spectrogram(audio_file, spec_params):\n",
    "    sr, hop_length, n_fft, n_mels, fmin, fmax = [\n",
    "        spec_params[k] for k in [\"sr\", \"hop_length\", \"n_fft\", \"n_mels\", \"fmin\", \"fmax\"]\n",
    "    ]\n",
    "    audio, _ = librosa.core.load(audio_file, sr=sr, mono=True)\n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        audio,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        power=1,\n",
    "    )\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def pcen_bird(melspec, **spec_params):\n",
    "    \"\"\"\n",
    "    parameters are taken from [1]:\n",
    "        - [1] Lostanlen, et. al. Per-Channel Energy Normalization: Why and How. IEEE Signal Processing Letters, 26(1), 39-43.\n",
    "    \"\"\"\n",
    "    sr, hop_length = [spec_params[k] for k in [\"sr\", \"hop_length\"]]\n",
    "    return librosa.pcen(\n",
    "        melspec * (2 ** 31),\n",
    "        time_constant=0.06,\n",
    "        eps=1e-6,\n",
    "        gain=0.8,\n",
    "        power=0.25,\n",
    "        bias=10,\n",
    "        sr=sr,\n",
    "        hop_length=hop_length,\n",
    "    )\n",
    "\n",
    "\n",
    "def mel2audio(melspec, **spec_params):\n",
    "    n_fft, sr, hop_length = [spec_params[k] for k in [\"n_fft\", \"sr\", \"hop_length\"]]\n",
    "    return librosa.feature.inverse.mel_to_audio(\n",
    "        melspec, sr=sr, n_fft=n_fft, hop_length=hop_length, power=1\n",
    "    )\n",
    "\n",
    "\n",
    "def get_fullpath(filename, audio_path=\"../input/birdclef-2022/train_audio\"):\n",
    "    return f\"{audio_path}/{filename}\"\n",
    "\n",
    "\n",
    "def play_audio(audio_file):\n",
    "    display(ipd.Audio(audio_file))\n",
    "\n",
    "\n",
    "def gen_spec_and_audio(audio_name):\n",
    "    out_file = audio_name.replace(\"/\", \"_\")[:-4] + \".wav\"\n",
    "    img = plot_spectrograms(audio_name)\n",
    "    print(\"source audio:\")\n",
    "    play_audio(get_fullpath(audio_name))\n",
    "    return img #Inserted return for PCEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54149af2-8834-4ed9-881b-e1a81f411635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_for_clip(test_df: pd.DataFrame,  #BUCKAROO\n",
    "                        clip: np.ndarray, \n",
    "                        models, \n",
    "                        threshold=0.05, \n",
    "                        threshold_long=None):\n",
    "\n",
    "    dataset = TestDataset(df=test_df, \n",
    "                          clip=clip,\n",
    "                          train_period = CFG.period, \n",
    "                          waveform_transforms=get_transforms(phase=\"test\"))\n",
    "    loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     [model.eval() for model in models]\n",
    "    prediction_dict = {}\n",
    "    mel_model.to(device)\n",
    "    for image, row_id in tqdm(loader):\n",
    "        row_id = row_id[0]\n",
    "        #image = create_mel_spectrogram(image,spec_params)\n",
    "        image = image.to(device)\n",
    "\n",
    "        with torch.no_grad(): #(mel_model.model.logmelspec_extractor[0](image)[:, None]\n",
    "            image =  mel_model.model.logmelspec_extractor[0](image)[:, None]#mel_model[0].logmelspec_extractor(image)[:, None]#image = create_mel_spectrogram(image,spec_params) #models[0]#.logmelspec_extractor(image)[:, None]\n",
    "            #conver to image for Inc3\n",
    "            #tensor  = tensor.cpu().numpy() # make sure tensor is on cpu\n",
    "            #cv2.imwrite(tensor, \"image.png\")\n",
    "            #image = T.ToPILImage(image)\n",
    "            print(image.shape)\n",
    "            numpy_img = image.permute(1,2,0).cpu().numpy()\n",
    "            image = Image.fromarray((numpy_img * 255).astype(np.uint8))\n",
    "            probas = []\n",
    "            probas_long = []\n",
    "            #for model in models:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                _, clipwise_pred, _, _, clipwise_pred_long = newmodel(image)\n",
    "            probas.append(clipwise_pred.detach().cpu().numpy().reshape(-1))\n",
    "            probas_long.append(clipwise_pred_long.detach().cpu().numpy().reshape(-1))\n",
    "            probas = np.array(probas)\n",
    "            probas_long = np.array(probas_long)\n",
    "#             probas = np.array([model(image)[1].detach().cpu().numpy().reshape(-1) for model in models])\n",
    "        if threshold_long is None:\n",
    "            events = probas.mean(0) >= threshold\n",
    "        else:\n",
    "            events = ((probas.mean(0) >= threshold).astype(int) \\\n",
    "                      + (probas_long.mean(0) >= threshold_long).astype(int)) >= 2\n",
    "        labels = np.argwhere(events).reshape(-1).tolist()\n",
    "#         labels = labels[:2]\n",
    "        if len(labels) == 0:\n",
    "            prediction_dict[str(row_id)] = \"nocall\"\n",
    "        else:\n",
    "            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n",
    "            label_string = \" \".join(labels_str_list)\n",
    "            prediction_dict[str(row_id)] = label_string\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "217c9144-f5ed-4ec8-b737-523593230a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone=\"inception_v3\", #resnet34\n",
    "        p=0.5,\n",
    "        n_mels=128,\n",
    "        num_class=CFG.num_classes,\n",
    "        train_period=CFG.period,\n",
    "        infer_period=5.0,\n",
    "        in_chans=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        #self.model = AttModel(backbone, p, n_mels, num_class, train_period, infer_period, in_chans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4ac4a-b7ca-4fcb-803d-3cfa581428d3",
   "metadata": {},
   "source": [
    "## constructor for model + predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314db8df-69b3-41c2-a470-8aa8e96c5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(backbone_name, weight_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Model(\n",
    "        backbone_name,\n",
    "        p=0.5,\n",
    "        n_mels=CFG.n_mels,\n",
    "        num_class=CFG.num_classes,\n",
    "        train_period=CFG.period,\n",
    "        infer_period=5,\n",
    "    )\n",
    "    model = model_x\n",
    "    #model = prepare_model_for_inference(model, weight_path).to(device)\n",
    "    #model = model.model\n",
    "    return model\n",
    "\n",
    "def prediction(test_audios, \n",
    "               models_cfg,\n",
    "               threshold=0.05, \n",
    "               threshold_long=None):\n",
    "    \n",
    "    #models = [load_model(list(models_cfg.keys())[0], list(models_cfg.values())[0]) for models_cfg in models_cfg]\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    prediction_dicts = {}\n",
    "    for audio_path in test_audios:\n",
    "        with timer(f\"Loading {str(audio_path)}\", logger):\n",
    "            clip, _ = sf.read(audio_path)\n",
    "        seconds = []\n",
    "        row_ids = []\n",
    "        for second in range(5, 65, 5):\n",
    "            row_id = \"_\".join(audio_path.name.split(\".\")[:-1]) + f\"_{second}\"\n",
    "            seconds.append(second)\n",
    "            row_ids.append(row_id)\n",
    "        print(row_ids)\n",
    "        test_df = pd.DataFrame({\n",
    "            \"row_id\": row_ids,\n",
    "            \"seconds\": seconds\n",
    "        })\n",
    "        with timer(f\"Prediction on {audio_path}\", logger):\n",
    "            prediction_dict = prediction_for_clip(test_df,\n",
    "                                                  clip=clip,\n",
    "                                                  models=models,\n",
    "                                                  threshold=threshold, threshold_long=threshold_long)\n",
    "#         row_id = list(prediction_dict.keys())\n",
    "#         birds = list(prediction_dict.values())\n",
    "#         prediction_df = pd.DataFrame({\n",
    "#             \"row_id\": row_id,\n",
    "#             \"birds\": birds\n",
    "#         })\n",
    "#         prediction_dfs.append(prediction_df)\n",
    "#     prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n",
    "        prediction_dicts.update(prediction_dict)\n",
    "    return prediction_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cd5e1f1-5ce4-498c-9afa-0d771144ae79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load(\"tester2.pth\")\n",
    "#state_dict = torch.load('federated_mnist_best_test.pth')\n",
    "a = Model()#('inception_v3', \"tester2.pth\") # weights[\"state_dict\"]\n",
    "a.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2452dc0d-4f5d-4b74-8d8e-a40d50fb8440",
   "metadata": {},
   "source": [
    "## Todo, rewrite prediction dictionary creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3450df02-9be1-4eb0-a659-4ba933900111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_spectrograms(\n",
    "#     audio_name,\n",
    "#     spec_params=dict(\n",
    "#         sr=32_000, hop_length=320, n_fft=512, n_mels=128, fmin=20, fmax=14_000 #Changed n_fft=800, hop_lenght to frame (1 = 10ms)\n",
    "#     ),\n",
    "# ):\n",
    "#     sr, hop_length, fmin, fmax, n_mels = [\n",
    "#         spec_params[k] for k in [\"sr\", \"hop_length\", \"fmin\", \"fmax\", \"n_mels\"]\n",
    "#     ]\n",
    "#     print(f\"parameters: {spec_params}\")\n",
    "#     audio_file = get_fullpath(audio_name)\n",
    "#     if not os.path.isfile(audio_file):\n",
    "#         raise FileNotFoundError\n",
    "#     melspec = create_mel_spectrogram(audio_file, **spec_params)\n",
    "#     log_melspec = librosa.amplitude_to_db(melspec, ref=np.max)\n",
    "#     pcen_melspec = pcen_bird(melspec, **spec_params)\n",
    "    \n",
    "#     return pcen_melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "440c9dad-b150-4bae-9be9-3ec5eafad522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeMFreq(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = torch.nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem_freq(x, p=self.p, eps=self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5e5341d-c249-4e08-b5c7-207a7ec0052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_chans, p=0.5, num_class=152, train_period=15.0, infer_period=5.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_period = train_period\n",
    "        self.infer_period = infer_period\n",
    "        self.pooling = GeMFreq()\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(p / 2),\n",
    "            nn.Linear(in_chans, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.attention = nn.Conv1d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_class,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.fix_scale = nn.Conv1d(\n",
    "            in_channels=512,\n",
    "            out_channels=num_class,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, feat):\n",
    "        feat = self.pooling(feat).squeeze(-2).permute(0, 2, 1)  # (bs, time, ch)\n",
    "\n",
    "        feat = self.dense_layers(feat).permute(0, 2, 1)  # (bs, 512, time)\n",
    "        time_att = torch.tanh(self.attention(feat))\n",
    "        assert self.train_period >= self.infer_period\n",
    "        if self.training or self.train_period == self.infer_period:\n",
    "\n",
    "            clipwise_pred = torch.sum(\n",
    "                torch.sigmoid(self.fix_scale(feat)) * torch.softmax(time_att, dim=-1),\n",
    "                dim=-1,\n",
    "            )  # sum((bs, 24, time), -1) -> (bs, 24)\n",
    "            logits = torch.sum(\n",
    "                self.fix_scale(feat) * torch.softmax(time_att, dim=-1),\n",
    "                dim=-1,\n",
    "            )\n",
    "        else:\n",
    "            framewise_pred_long = torch.sigmoid(self.fix_scale(feat))\n",
    "            clipwise_pred_long = torch.sum(framewise_pred_long * torch.softmax(time_att, dim=-1), dim=-1) \n",
    "            \n",
    "            feat_time = feat.size(-1)\n",
    "            start = (\n",
    "                feat_time / 2 - feat_time * (self.infer_period / self.train_period) / 2\n",
    "            )\n",
    "            end = start + feat_time * (self.infer_period / self.train_period)\n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            feat = feat[:, :, start:end]\n",
    "            att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "#             print(feat_time, start, end)\n",
    "#             print(att_a.sum(), att.sum(), time_att.shape)\n",
    "            framewise_pred = torch.sigmoid(self.fix_scale(feat))\n",
    "            clipwise_pred = torch.sum(framewise_pred * att, dim=-1) \n",
    "            logits = torch.sum(\n",
    "                self.fix_scale(feat) * att,\n",
    "                dim=-1,\n",
    "            )\n",
    "            time_att = time_att[:, :, start:end]\n",
    "        return (\n",
    "            logits,\n",
    "            clipwise_pred,\n",
    "            self.fix_scale(feat).permute(0, 2, 1),\n",
    "            time_att.permute(0, 2, 1),\n",
    "            clipwise_pred_long,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcecacf1-ef7f-483e-bd8c-bee421e81ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean((1, 2), keepdim=True)\n",
    "        std = X.std((1, 2), keepdim=True)\n",
    "        Xstd = (X - mean) / (std + self.eps)\n",
    "        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n",
    "        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n",
    "            (norm_max - norm_min)\n",
    "        )\n",
    "        V = torch.zeros_like(Xstd)\n",
    "        if fix_ind.sum():\n",
    "            V_fix = Xstd[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind, None, None]\n",
    "            norm_min_fix = norm_min[fix_ind, None, None]\n",
    "            V_fix = torch.max(\n",
    "                torch.min(V_fix, norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n",
    "            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "            V[fix_ind] = V_fix\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d78e0638-dfdf-45e8-bec0-dfc105734aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone=\"resnet34\",\n",
    "        p=0.5,\n",
    "        n_mels=224,\n",
    "        num_class=152,\n",
    "        train_period=15.0,\n",
    "        infer_period=5.0,\n",
    "        in_chans=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.logmelspec_extractor = nn.Sequential(\n",
    "            MelSpectrogram(\n",
    "                32000,\n",
    "                n_mels=n_mels,\n",
    "                f_min=20,\n",
    "                n_fft=2048,\n",
    "                hop_length=512,\n",
    "                normalized=True,\n",
    "            ),\n",
    "            AmplitudeToDB(top_db=80.0),\n",
    "            NormalizeMelSpec(),\n",
    "        )\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone, features_only=True, pretrained=False, in_chans=in_chans\n",
    "        )\n",
    "        encoder_channels = self.backbone.feature_info.channels()\n",
    "        dense_input = encoder_channels[-1]\n",
    "        self.head = AttHead(\n",
    "            dense_input,\n",
    "            p=p,\n",
    "            num_class=num_class,\n",
    "            train_period=train_period,\n",
    "            infer_period=infer_period,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "#         img = self.logmelspec_extractor(input)[\n",
    "#             :, None\n",
    "#         ]  # (batch_size, 1, mel_bins, time_steps)\n",
    "        feats = self.backbone(input)\n",
    "        return self.head(feats[-1])\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone=\"resnet34\",\n",
    "        p=0.5,\n",
    "        n_mels=224,\n",
    "        num_class=CFG.num_classes,\n",
    "        train_period=CFG.period,\n",
    "        infer_period=5.0,\n",
    "        in_chans=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = AttModel(backbone, p, n_mels, num_class, train_period, infer_period, in_chans)    \n",
    "\n",
    "mel_model = Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ff269ce-f185-43aa-8847-6c89825694de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MelSpectrogram(\n",
      "  (spectrogram): Spectrogram()\n",
      "  (mel_scale): MelScale()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mel_model.model.logmelspec_extractor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f518eaac-f231-43ff-9077-a5df6833d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 12:32:28,276 - INFO - [Loading birdclef-2022/test_soundscapes/soundscape_453028782.ogg] start\n",
      "2023-03-28 12:32:28,307 - INFO - [Loading birdclef-2022/test_soundscapes/soundscape_453028782.ogg] done in 0.03 s\n",
      "2023-03-28 12:32:28,308 - INFO - [Prediction on birdclef-2022/test_soundscapes/soundscape_453028782.ogg] start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soundscape_453028782_5', 'soundscape_453028782_10', 'soundscape_453028782_15', 'soundscape_453028782_20', 'soundscape_453028782_25', 'soundscape_453028782_30', 'soundscape_453028782_35', 'soundscape_453028782_40', 'soundscape_453028782_45', 'soundscape_453028782_50', 'soundscape_453028782_55', 'soundscape_453028782_60']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                 | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 1876])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m spec_params \u001b[38;5;241m=\u001b[39m gen_specs()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#    print(f\"parameters: {spec_params}\")    \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#print(plot_spectrograms())\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m prediction_dicts \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_audios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_audios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmodels_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m           \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m           \u001b[49m\u001b[43mthreshold_long\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_long\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction_dicts)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sample_submission)):\n",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m, in \u001b[0;36mprediction\u001b[0;34m(test_audios, models_cfg, threshold, threshold_long)\u001b[0m\n\u001b[1;32m     34\u001b[0m         test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: row_ids,\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m: seconds\n\u001b[1;32m     37\u001b[0m         })\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, logger):\n\u001b[0;32m---> 39\u001b[0m             prediction_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_for_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold_long\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold_long\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#         row_id = list(prediction_dict.keys())\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#         birds = list(prediction_dict.values())\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#         prediction_df = pd.DataFrame({\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#         prediction_dfs.append(prediction_df)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#     prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         prediction_dicts\u001b[38;5;241m.\u001b[39mupdate(prediction_dict)\n",
      "Cell \u001b[0;32mIn[21], line 29\u001b[0m, in \u001b[0;36mprediction_for_clip\u001b[0;34m(test_df, clip, models, threshold, threshold_long)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#conver to image for Inc3\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#tensor  = tensor.cpu().numpy() # make sure tensor is on cpu\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#cv2.imwrite(tensor, \"image.png\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#image = T.ToPILImage(image)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 29\u001b[0m numpy_img \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     30\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray((numpy_img \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n\u001b[1;32m     31\u001b[0m probas \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "threshold = 0.025\n",
    "threshold_long = 0.05\n",
    "\n",
    "# def plot_spectrograms(\n",
    "#     spec_params=dict(\n",
    "#         sr=32_000, hop_length=320, n_fft=512, n_mels=128, fmin=20, fmax=14_000 #Changed n_fft=800, hop_lenght to frame (1 = 10ms)\n",
    "#     ),\n",
    "# ):\n",
    "#     sr, hop_length, fmin, fmax, n_mels = [\n",
    "#         spec_params[k] for k in [\"sr\", \"hop_length\", \"fmin\", \"fmax\", \"n_mels\"]\n",
    "#     ]\n",
    "#     print(f\"parameters: {spec_params}\")\n",
    "#     audio_file = get_fullpath(audio_name)\n",
    "#     if not os.path.isfile(audio_file):\n",
    "#         raise FileNotFoundError\n",
    "#     melspec = create_mel_spectrogram(audio_file, **spec_params)\n",
    "#     log_melspec = librosa.amplitude_to_db(melspec, ref=np.max)\n",
    "#     pcen_melspec = pcen_bird(melspec, **spec_params)\n",
    "    \n",
    "#def plot_spectrograms(\n",
    "def gen_specs(\n",
    "    spec_params=dict(\n",
    "        sr=32_000, hop_length=320, n_fft=512, n_mels=128, fmin=20, fmax=14_000 #Changed n_fft=800, hop_lenght to frame (1 = 10ms)\n",
    "    ),\n",
    "):\n",
    "    sr, hop_length, fmin, fmax, n_mels = [\n",
    "        spec_params[k] for k in [\"sr\", \"hop_length\", \"fmin\", \"fmax\", \"n_mels\"]\n",
    "    ]\n",
    "    return spec_params\n",
    "    \n",
    "spec_params = gen_specs()\n",
    "#    print(f\"parameters: {spec_params}\")    \n",
    "#print(plot_spectrograms())\n",
    "\n",
    "\n",
    "prediction_dicts = prediction(test_audios=all_audios,\n",
    "           models_cfg=CFG.models_cfg,\n",
    "           threshold=threshold, \n",
    "           threshold_long=threshold_long)\n",
    "print(prediction_dicts)\n",
    "\n",
    "for i in range(len(sample_submission)):\n",
    "    sample = sample_submission.row_id[i]\n",
    "    key = sample.split(\"_\")[0] + \"_\" + sample.split(\"_\")[1] + \"_\" + sample.split(\"_\")[3]\n",
    "    target_bird = sample.split(\"_\")[2]\n",
    "    print(key, target_bird)\n",
    "    if key in prediction_dicts:\n",
    "        sample_submission.iat[i, 1] = (target_bird in prediction_dicts[key])\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "# submission = prediction(test_audios=all_audios,\n",
    "#                         models_cfg=CFG.models_cfg,\n",
    "#                         threshold=threshold, \n",
    "#                         threshold_long=threshold_long)\n",
    "# submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b67b0-3055-4ed1-850b-317c30d4e459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e85c1-186a-4ac1-8dcc-0fd02d2991d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
